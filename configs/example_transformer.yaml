# Example Transformer configuration
model:
  name: "simple_transformer"
  vocab_size: 1000
  d_model: 256
  num_heads: 8
  num_layers: 4
  dim_feedforward: 1024
  max_seq_length: 128
  num_classes: 2
  dropout: 0.1

data:
  name: "dummy_text"
  num_samples: 1000
  seq_length: 128
  vocab_size: 1000
  num_classes: 2
  train:
    num_samples: 800
    seq_length: 128
    vocab_size: 1000
    num_classes: 2
  validation:
    num_samples: 200
    seq_length: 128
    vocab_size: 1000
    num_classes: 2

training:
  num_epochs: 20
  batch_size: 16
  learning_rate: 0.0001
  weight_decay: 1e-4
  optimizer: "adamw"
  scheduler: "cosine"
  loss: "cross_entropy"
  gradient_clip: 1.0
  save_interval: 5
  eval_interval: 1
  log_interval: 10
  use_tensorboard: true

experiment:
  seed: 42
  output_dir: "./outputs/transformer_experiment"
  device: "auto"

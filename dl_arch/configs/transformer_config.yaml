# Default configuration template for Transformer model
model:
  type: "transformer"
  vocab_size: 10000
  seq_length: 512
  d_model: 512
  num_heads: 8
  num_layers: 6
  d_ff: 2048
  output_dim: 10000
  dropout: 0.1

data:
  type: "text"  # or sequence
  data_path: "./data"
  vocab_path: "./data/vocab.txt"
  max_length: 512
  batch_size: 16
  num_workers: 4
  shuffle: true
  pin_memory: true

training:
  num_epochs: 50
  learning_rate: 0.0001
  weight_decay: 0.01
  optimizer: "adamw"
  scheduler: "cosine"
  gradient_clip: 1.0
  save_every: 5
  eval_every: 1

evaluation:
  task_type: "classification"  # or "generation"

experiment:
  name: "transformer_experiment"
  output_dir: "./outputs"
  seed: 42
  device: "auto"

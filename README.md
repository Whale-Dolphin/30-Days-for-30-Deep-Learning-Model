# DL-Arch: Universal Deep Learning Architecture Framework

## Overview

A modular and extensible deep learning framework supporting CNN, Transformer, MLP, and other architectures with unified training and evaluation pipelines.

## Recent Optimizations (User Rules Compliance)

### ğŸ”§ Code Documentation Standards
- âœ… Added detailed tensor operation comments with input/output shapes
- âœ… Included mathematical formulas using LaTeX notation in comments  
- âœ… Added purpose descriptions for all tensor operations
- âœ… All comments and docstrings are in English

### ğŸ“Š Logging Standards  
- âœ… Replaced Python's built-in logging with **loguru**
- âœ… Added structured logging with meaningful messages
- âœ… Implemented debug logging for tensor shapes and operations
- âœ… Added gradient norms and parameter monitoring

### âš¡ Deep Learning Framework Optimizations
- âœ… Enhanced PyTorch device detection: **CUDA > MPS > CPU** priority
- âœ… Added explicit device placement with Apple MPS support
- âœ… Implemented proper tensor dtype specifications
- âœ… Added tensor shape validation and assertions

### ğŸ“ˆ Training and Evaluation Monitoring
- âœ… Enhanced TensorBoard integration with:
  - Training/validation losses and metrics
  - Learning rate tracking
  - Gradient norm monitoring
  - Parameter histogram logging
  - Structured tag naming

### ğŸ›¡ï¸ Error Handling
- âœ… Added proper error handling for tensor operations
- âœ… Implemented tensor shape and dtype validation
- âœ… Added assertions for critical tensor dimension assumptions
- âœ… Enhanced error logging with sufficient debugging context

### ğŸ“ Code Style
- âœ… Added type hints for function parameters and return values
- âœ… Implemented meaningful variable names and docstrings
- âœ… Following PEP 8 style guidelines

## Architecture Support

### Models
- **SimpleCNN**: Convolutional Neural Network for image classification
- **SimpleMLP**: Multi-Layer Perceptron for tabular data
- **SimpleTransformer**: Transformer encoder for sequence classification

### Data Types
- **Image Data**: Via `dummy_image` dataset
- **Text Data**: Via `dummy_text` dataset  
- **Tabular Data**: Via `dummy_tabular` dataset
- **Streaming Data**: Support for large-scale datasets

## Quick Start

```bash
# Train MLP model
python main.py --config configs/example_mlp.yaml --mode train

# Train CNN model  
python main.py --config configs/example_cnn.yaml --mode train

# Train Transformer model
python main.py --config configs/example_transformer.yaml --mode train

# Evaluate trained model
python main.py --config configs/example_mlp.yaml --mode eval --resume outputs/mlp_experiment/best_model.pth

# Full training + evaluation
python main.py --config configs/example_transformer.yaml --mode both
```

## Device Support

The framework automatically detects and uses the best available device:

1. **CUDA** (NVIDIA GPUs) - First priority
2. **MPS** (Apple Metal) - Second priority  
3. **CPU** - Fallback option

## Logging and Monitoring

### Loguru Integration
- Structured logging with rotation
- Debug-level tensor shape tracking
- Error context preservation

### TensorBoard Features
- Real-time training metrics
- Gradient norm monitoring
- Parameter histogram visualization
- Learning rate tracking

## Key Features

- ğŸ”„ **Modular Registry System**: Easy model and dataset registration
- ğŸ“Š **Comprehensive Metrics**: Built-in evaluation with multiple metrics
- ğŸ’¾ **Checkpoint Management**: Automatic model saving and resuming
- ğŸ¯ **Multi-Task Support**: Classification and regression tasks
- ğŸš€ **GPU Acceleration**: Automatic device detection and optimization
- ğŸ“ˆ **Visualization**: TensorBoard integration for training monitoring

## Project Structure

```
dl_arch/
â”œâ”€â”€ data/           # Dataset classes and data loading utilities
â”œâ”€â”€ models/         # Model architectures and base classes
â”œâ”€â”€ training/       # Training loop and optimization
â”œâ”€â”€ evaluation/     # Evaluation metrics and tools
â””â”€â”€ utils/          # Utilities and helper functions

configs/            # YAML configuration files
outputs/            # Training outputs and checkpoints
```

## Configuration

All experiments are configured via YAML files with support for:
- Model hyperparameters
- Training settings
- Data configuration
- Device selection
- Logging preferences

## Contributing

This framework follows strict code quality standards:
- Type hints for all functions
- Comprehensive tensor operation documentation
- Structured logging throughout
- Proper error handling with context
